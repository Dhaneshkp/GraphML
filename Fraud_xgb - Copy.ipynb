{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"include_colab_link":true},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10328793,"sourceType":"datasetVersion","datasetId":6395417},{"sourceId":10467187,"sourceType":"datasetVersion","datasetId":6480710},{"sourceId":10521528,"sourceType":"datasetVersion","datasetId":6511809}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/Dhaneshkp/GraphML/blob/main/Fraud_xgb%20-%20Copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"id":"6e127a6c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"!pip install --upgrade certifi\n\n!pip install --trusted-host files.pythonhosted.org optuna\n\n","metadata":{"id":"cca9ea61"}},{"cell_type":"code","source":"#os.listdir(\"/kaggle/input/fraud-detection\")","metadata":{"id":"2ae5c024","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/fraud-detection/fraudTrain.csv\")","metadata":{"id":"13e8ce13","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef are_cc_numbers_unique(df, cc_column):\n\n    try:\n      # Check if the specified column exists in the DataFrame.\n      if cc_column not in df.columns:\n          print(f\"Error: Column '{cc_column}' not found in the DataFrame.\")\n          return False\n\n      return df[cc_column].nunique() == len(df)\n\n    except Exception as e:\n      print(f\"An error occurred: {e}\")\n      return False\n\n\nare_unique = are_cc_numbers_unique(data, 'cc_num')\n\nif are_unique:\n     print(\"All credit card numbers are unique.\")\nelse:\n     print(\"Some credit card numbers are duplicated.\")","metadata":{"id":"EeWV_J0t74Us","outputId":"93fd8b16-ea09-4ab6-de77-503548d7b940","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport datetime\n\ndata['trans_date_trans_time'] = pd.to_datetime(data['trans_date_trans_time'])\n\n# Extract the time of day as an integer representing minutes since midnight\ndata['time_of_day'] = (data['trans_date_trans_time'].dt.hour * 60) + data['trans_date_trans_time'].dt.minute\n\n\ndata['time_of_day_category'] = pd.cut(data['time_of_day'], bins=[0, 6*60, 12*60, 18*60, 24*60],\n                                      labels=['Night', 'Morning', 'Afternoon', 'Evening'], right=False)\n\n\ndata.head()","metadata":{"id":"po4tbvvL2mBu","outputId":"e1e289d0-7827-4ec1-f939-e7b16400da17","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndata['unix_time_c'] = pd.to_datetime(data['unix_time'], unit='s')\n\n# Extract the time of day as an integer representing minutes since midnight\ndata['time_of_day_uc'] = (data['unix_time_c'].dt.hour * 60) + data['unix_time_c'].dt.minute\n\ndata['time_of_day_category_uc'] = pd.cut(data['time_of_day_uc'], bins=[0, 6*60, 12*60, 18*60, 24*60],\n                                      labels=['Night', 'Morning', 'Afternoon', 'Evening'], right=False)\n\ndata.head()","metadata":{"id":"Vx2fexCW7GDg","outputId":"5d34760d-d3bb-40ee-cbad-3df621c0c3bb","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.columns","metadata":{"id":"ad7c98b3","outputId":"9a60a7c3-0045-444d-a920-f7153a94a875","scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.describe()","metadata":{"id":"c6f27470","outputId":"dc7c595f-79cd-43d8-842f-ef479b8f436a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.describe(include='object')","metadata":{"id":"ec94315d","outputId":"2bbb9627-cd12-4894-e417-6144ead6b4a1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = data.drop(['Unnamed: 0', 'trans_date_trans_time','trans_num', 'unix_time'],axis=1)\n","metadata":{"id":"002450e0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df=train_df.sample(frac=0.5,random_state=1)# if the data us indded by a person or a cc num then this sampling isnt appropriate","metadata":{"id":"3f8Lm9DqqzSK","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"v84qETkr9Zlt","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['distance']= (train_df['lat'] - train_df['merch_lat'])**2 + (train_df['long'] - train_df['merch_long'])**2","metadata":{"id":"d3bfb74c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n    text = ' '.join([word for word in text.split() if word not in stopwords.words('english') and not word.isdigit()])\n    return text\n\n# Apply cleaning to relevant columns\n#train_df['merchant'] = train_df['merchant'].apply(clean_text)\n#train_df['category'] = train_df['category'].apply(clean_text)\ntrain_df['city_state'] = train_df['city'] + '_' + train_df['state']","metadata":{"id":"6c1a21da","outputId":"939ea352-896d-4389-833f-6557a03d9b92","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['city_state'] = train_df['city'] + '_' + train_df['state']","metadata":{"id":"737d7dfa","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datetime import datetime\n\n# Function to calculate age\ndef calculate_age(dob):\n    dob = str(dob)\n    today = datetime.today()\n    dob = datetime.strptime(dob, '%Y-%m-%d')\n    age = today.year - dob.year - ((today.month, today.day) < (dob.month, dob.day))\n    return age\n\n# Apply the function to the 'dob' column\ntrain_df['age'] = data['dob'].apply(calculate_age)\n","metadata":{"id":"152bbec2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train_df = train_df.dropna(subset=['dob'])","metadata":{"id":"dReaIvq3ApvO","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"categorical_columns = ['category', 'gender', 'job','city','state','merchant','street','city_state']\nfor column in categorical_columns:\n    train_df[column] = train_df[column].astype('category')","metadata":{"id":"dafbcc9e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.dtypes","metadata":{"id":"8e08f7ab","outputId":"a833874d-a63e-48b6-d74e-e272b53c84db","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.drop(['merch_lat', 'merch_long','lat', 'long'],axis=1,inplace=True)","metadata":{"id":"b973f56e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.columns","metadata":{"id":"e85e852e","outputId":"05142913-afab-4953-a639-31e3048328c4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.drop(['street','zip','city_pop'],axis=1,inplace=True)","metadata":{"id":"fff694bf","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train_df.drop(['dob'],axis=1,inplace=True)","metadata":{"id":"210cafd8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train_df.to_csv(\"train_df_new.csv\")","metadata":{"id":"9af53317","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train_df=pd.read_csv(\"train_df_new.csv\")\n#train_df.drop('Unnamed: 0',axis=1,inplace=True)","metadata":{"id":"ea8779c3","scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pykeen","metadata":{"id":"iebAgA3ULoud","outputId":"790e66d1-4958-449a-fb67-b7558cc725e9","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import networkx as nx\n\nG = nx.Graph()\n\n# Add nodes and edges based on the data\nfor index, row in train_df.iterrows():\n    trans_id = f'trans_{index}'\n    G.add_node(trans_id, label='transaction')\n    G.add_node(row['merchant'], label='merchant')\n    G.add_node(row['category'], label='category')\n    G.add_node(row['city'], label='city')\n    G.add_node(row['state'], label='state')\n    G.add_edge(trans_id, row['merchant'], relationship='made_at')\n    G.add_edge(trans_id, row['category'], relationship='belongs_to')\n    G.add_edge(trans_id, row['city'], relationship='located_in')\n    G.add_edge(trans_id, row['state'], relationship='in')\n    G.add_node(row['time_of_day_category'] , label='time')\n    G.add_edge(trans_id, row['time_of_day_category'] , relationship='time')\n    #G.add_node(row['cc_num'], label='cc_num')\n    #G.add_edge(trans_id, row['cc_num'], relationship='card')\n    G.add_node(row['job'], label='job')\n    G.add_edge(trans_id, row['job'], relationship='job')","metadata":{"id":"690677af","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pykeen.pipeline import pipeline\nfrom pykeen.triples import TriplesFactory\nfrom sklearn.model_selection import train_test_split\nimport torch\n# Create triples for KG (subject, predicate, object)\ntriples = []\nfor index, row in train_df.iterrows():\n    trans_id = f'trans_{index}'\n    triples.append((trans_id, 'made_at', row['merchant']))\n    triples.append((trans_id, 'belongs_to', row['category']))\n    triples.append((trans_id, 'located_in', row['city']))\n    triples.append((trans_id, 'in', row['state']))\n    triples.append((trans_id, 'in', row['time_of_day_category']))\n    triples.append((trans_id, 'job', row['job']))\n    #triples.append((trans_id, 'card', row['cc_num']))\ntriples = np.array(triples)\n# Create a PyKEEN dataset\nfor i in range(25):\n    print(triples[i])\n\n# Check data types of the triples\nprint(type(triples[0][0]), type(triples[0][1]), type(triples[0][2]))\n\n# Remove malformed entries\nvalid_triples = [triple for triple in triples if len(triple) == 3]\ntriples = np.array(valid_triples)\n\n# Split the triples into training and testing sets\ntrain_triples, test_triples = train_test_split(triples, test_size=0.2, random_state=42)\ntrain_triples=triples\n# Create TriplesFactory objects for training and testing sets\ntrain_tf = TriplesFactory.from_labeled_triples(train_triples)\ntest_tf = TriplesFactory.from_labeled_triples(test_triples)\n\n# Print TriplesFactory details for debugging\nprint(train_tf)\nprint(test_tf)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Train a KG embedding model with both training and testing triples factories\nresult = pipeline(\n    training=train_tf,\n    testing=test_tf,\n    model='TransE',\n    training_kwargs=dict(num_epochs=10),\n)\nall_entities = train_tf.entity_to_id\nembeddings = result.model.entity_representations[0](indices=torch.arange(train_tf.num_entities, device=result.model.device)).cpu().detach().numpy()\n\n# Create a DataFrame with embeddings\nembedding_df = pd.DataFrame(embeddings, index=all_entities.keys())\n\n# Merge embeddings with labels\ntrain_df['embedding'] = train_df.index.map(lambda x: embedding_df.loc[f'trans_{x}'].values)","metadata":{"id":"64dbbeac","outputId":"0fafdbf7-a4eb-430b-b998-b7176900433c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train_df.to_csv(\"train_df_new.csv\")","metadata":{"execution":{"iopub.status.busy":"2025-01-20T04:28:00.271076Z","iopub.execute_input":"2025-01-20T04:28:00.271369Z","iopub.status.idle":"2025-01-20T04:28:00.276047Z","shell.execute_reply.started":"2025-01-20T04:28:00.271345Z","shell.execute_reply":"2025-01-20T04:28:00.274572Z"},"trusted":true,"id":"Keb36WSiRPRH"},"outputs":[],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(result.model.state_dict(), 'transe_model.pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:28:00.278215Z","iopub.execute_input":"2025-01-20T04:28:00.278660Z","iopub.status.idle":"2025-01-20T04:28:00.388728Z","shell.execute_reply.started":"2025-01-20T04:28:00.278625Z","shell.execute_reply":"2025-01-20T04:28:00.385801Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-df368622852b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'transe_model.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"],"ename":"NameError","evalue":"name 'torch' is not defined","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"#train_df['embedding']=pd.read_csv(\"train_df_new_W_EMBEDDINGS.csv\")['embedding']\ntrain_df['embedding']=pd.read_csv(\"/kaggle/input/train-df-jan13-embeddings/train_df_new_13Jan2024.csv\")['embedding']","metadata":{"id":"S90gKAw5X7w5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install python-louvain\n\nfrom community import community_louvain\n\n# Calculate the Louvain community structure\npartition = community_louvain.best_partition(G)\n\n# Add the community assignments to the dataframe\ntrain_df['community'] = train_df.index.map(lambda x: partition.get(f'trans_{x}', -1))\n\n# Print or further analyze the community assignments\nprint(train_df[['community']])","metadata":{"id":"qP39Fq-HEjec","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.info()","metadata":{"id":"cDWi7p36CbN5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.dropna()","metadata":{"id":"s18wN9iHJSuE","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in train_df.columns:\n    if train_df[col].dtype == 'object' and col != 'embedding':  # Skip 'embedding' column\n        train_df[col] = train_df[col].astype('category')","metadata":{"id":"JN2GGx-FJIR8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df=train_df.dropna()","metadata":{"id":"cS6vgVtT08a1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"categorical_features = ['gender', 'community', 'time_of_day_category', 'job']\nfor col in categorical_features:\n    train_df[col] = train_df[col].astype('category')","metadata":{"trusted":true,"id":"JeXDxlSmRPQ6"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.describe()","metadata":{"id":"t1U8SPQaez-d","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train_df.to_csv(\"train_df_new.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:28:00.412069Z","iopub.execute_input":"2025-01-20T04:28:00.412506Z","iopub.status.idle":"2025-01-20T04:28:00.418483Z","shell.execute_reply.started":"2025-01-20T04:28:00.412466Z","shell.execute_reply":"2025-01-20T04:28:00.416790Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\n# Fit and transform the 'gender' column in both training and validation data\ntrain_df['gender_encoded'] = le.fit_transform(train_df['gender'])\n\n\nX =  train_df[['amt', 'distance', 'age', 'gender_encoded','community','time_of_day_category','job']]#'predicted_labels'\ny = train_df['is_fraud']\n\n","metadata":{"id":"a84a5a8f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# One-hot encode categorical features\nX_encoded = pd.get_dummies(X, columns=['time_of_day_category'])#, 'job'])\nfeatures = ['gender_encoded', 'community']\nfor col in features:\n    X_encoded[col] = X_encoded[col].astype('int')\n\n# Split the data\nX_train, X_val, y_train, y_val = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n\n# Initialize the XGBoost model\nmodel = xgb.XGBClassifier(random_state=42,enable_categorical=True)\n\n# Perform cross-validation\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(model, X_train, y_train, cv=cv, scoring='f1')\n\nprint(f\"Cross-Validation F1 Scores: {scores}\")\nprint(f\"Mean F1 Score: {scores.mean()}\")\n\n# Train the model on the training data\nmodel.fit(X_train, y_train)\n\n# Predict on the validation set\ny_val_pred = model.predict(X_val)\n\n# Evaluate the model\nprint(f\"Classification Report:\\n{classification_report(y_val, y_val_pred)}\")\nprint(f\"Confusion Matrix:\\n{confusion_matrix(y_val, y_val_pred)}\")\n\n# Plot feature importance\nxgb.plot_importance(model)\nplt.title('Feature Importance')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2025-01-20T04:28:01.918720Z","iopub.status.idle":"2025-01-20T04:28:01.919171Z","shell.execute_reply":"2025-01-20T04:28:01.918982Z"},"trusted":true,"id":"mUFf4t1VRPRJ"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_fscore_support, precision_score, recall_score, f1_score\n\ny_val_pred_proba = model.predict_proba(X_val)[:, 1]\ncutoffs = np.linspace(0, 1, 101)\nprecision = []\nrecall = []\nf1 = []\n\nfor cutoff in cutoffs:\n    y_val_pred = (y_val_pred_proba >= cutoff).astype(int)\n    precision.append(precision_score(y_val, y_val_pred))\n    recall.append(recall_score(y_val, y_val_pred))\n    f1.append(f1_score(y_val, y_val_pred))\n\nplt.figure(figsize=(10, 6))\nplt.plot(cutoffs, precision, label='Precision')\nplt.plot(cutoffs, recall, label='Recall')\nplt.plot(cutoffs, f1, label='F1 Score')\nplt.xlabel('Cutoff Probability')\nplt.ylabel('Score')\nplt.title('Precision, Recall, and F1 Score vs. Cutoff Probability')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\nbest_cutoff = cutoffs[np.argmax(f1)]\nprint(f\"Best cutoff for F1 Score: {best_cutoff}\")\n\ny_val_pred = (y_val_pred_proba >= best_cutoff).astype(int)\nprint(classification_report(y_val,y_val_pred))\n\n\nxgb.plot_importance(model)\nplt.title('Feature Importance')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-01-20T04:28:01.920405Z","iopub.status.idle":"2025-01-20T04:28:01.921178Z","shell.execute_reply":"2025-01-20T04:28:01.920714Z"},"id":"Jl3J-a1i0S23","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/fraud-detection/fraudTest.csv')","metadata":{"execution":{"iopub.status.busy":"2025-01-20T04:28:01.922394Z","iopub.status.idle":"2025-01-20T04:28:01.922817Z","shell.execute_reply":"2025-01-20T04:28:01.922669Z"},"id":"Cxnd-V4El7x2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.columns","metadata":{"execution":{"iopub.status.busy":"2025-01-20T04:28:01.923476Z","iopub.status.idle":"2025-01-20T04:28:01.923905Z","shell.execute_reply":"2025-01-20T04:28:01.923710Z"},"trusted":true,"id":"tswg-1w8RPRL"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom pykeen.models import TransE\nfrom pykeen.triples import TriplesFactory\n\n# Initialize the model with the same parameters used during training\nmodel = TransE(\n    triples_factory=train_tf,  # Add the triples_factory argument\n    embedding_dim=50,  # Ensure this matches the embedding dimension used during training\n)\n\n\n# Load the saved state dictionary\nmodel.load_state_dict(torch.load('transe_model.pt'))\n\n# Move the model to GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\n# Example: Get embeddings for a specific entity\nentity_id = train_tf.entity_to_id['trans_0']  # Replace 'trans_0' with your entity ID\nentity_embedding = model.entity_representations[0]\n\nprint(entity_embedding)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:28:01.925483Z","iopub.status.idle":"2025-01-20T04:28:01.925917Z","shell.execute_reply":"2025-01-20T04:28:01.925766Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TransE_model=model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:28:01.926612Z","iopub.status.idle":"2025-01-20T04:28:01.927071Z","shell.execute_reply":"2025-01-20T04:28:01.926877Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import networkx as nx\nfrom community import community_louvain\ntest_df['trans_date_trans_time'] = pd.to_datetime(test_df['trans_date_trans_time'])\ntest_df['time_of_day'] = (test_df['trans_date_trans_time'].dt.hour * 60) + test_df['trans_date_trans_time'].dt.minute\ntest_df['time_of_day_category'] = pd.cut(test_df['time_of_day'], bins=[0, 6*60, 12*60, 18*60, 24*60], labels=['Night', 'Morning', 'Afternoon', 'Evening'], right=False)\ntest_df['unix_time_c'] = pd.to_datetime(test_df['unix_time'], unit='s')\ntest_df['time_of_day_uc'] = (test_df['unix_time_c'].dt.hour * 60) + test_df['unix_time_c'].dt.minute\ntest_df['time_of_day_category_uc'] = pd.cut(test_df['time_of_day_uc'], bins=[0, 6*60, 12*60, 18*60, 24*60], labels=['Night', 'Morning', 'Afternoon', 'Evening'], right=False)\n\ntest_df = test_df.drop(['Unnamed: 0', 'trans_date_trans_time','trans_num', 'unix_time'],axis=1)\ntest_df['distance']= (test_df['lat'] - test_df['merch_lat'])**2 + (test_df['long'] - test_df['merch_long'])**2\n#test_df['merchant'] = test_df['merchant'].apply(clean_text)\n#test_df['category'] = test_df['category'].apply(clean_text)\ntest_df['city_state'] = test_df['city'] + '_' + test_df['state']\ntest_df = test_df.dropna(subset=['dob'])\ntest_df['age'] = test_df['dob'].apply(calculate_age)\n\ncategorical_cols = ['category', 'gender', 'job','city','state','merchant','street','city_state']\nfor col in categorical_cols:\n    test_df[col] = test_df[col].astype('category')\n\ntest_df.drop(['merch_lat', 'merch_long','lat', 'long','street','zip','city_pop','dob'],axis=1,inplace=True)\n\n\n# Create graph for test data\nG_test = nx.Graph()\nfor index, row in test_df.iterrows():\n    trans_id = f'trans_{index}'\n    G_test.add_node(trans_id, label='transaction')\n    G_test.add_node(row['merchant'], label='merchant')\n    G_test.add_node(row['category'], label='category')\n    G_test.add_node(row['city'], label='city')\n    G_test.add_node(row['state'], label='state')\n    G_test.add_edge(trans_id, row['merchant'], relationship='made_at')\n    G_test.add_edge(trans_id, row['category'], relationship='belongs_to')\n    G_test.add_edge(trans_id, row['city'], relationship='located_in')\n    G_test.add_edge(trans_id, row['state'], relationship='in')\n    G_test.add_node(row['time_of_day_category'], label='time')\n    G_test.add_edge(trans_id, row['time_of_day_category'], relationship='time')\n    #G_test.add_node(row['cc_num'], label='cc_num')\n    #G_test.add_edge(trans_id, row['cc_num'], relationship='card')\n    G_test.add_node(row['job'], label='job')\n    G_test.add_edge(trans_id, row['job'], relationship='job')\n# Community detection for test data\npartition_test = community_louvain.best_partition(G_test)\ntest_df['community'] = test_df.index.map(lambda x: partition_test.get(f'trans_{x}', -1))\n\n# Label Propagation for test data (using the trained model)\ntransaction_nodes_test = [node for node in G_test.nodes() if str(node).startswith('trans_')]\n#graph_laplacian_test = nx.laplacian_matrix(G_test, nodelist=transaction_nodes_test).toarray()\n#predicted_labels_test = label_prop_model.predict(graph_laplacian_test) # Use the trained model here\n#test_df['predicted_labels'] = predicted_labels_test\n","metadata":{"id":"tnOYFgtPRPRM","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:28:01.928012Z","iopub.status.idle":"2025-01-20T04:28:01.928470Z","shell.execute_reply":"2025-01-20T04:28:01.928279Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(dir(model))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:28:01.929071Z","iopub.status.idle":"2025-01-20T04:28:01.929406Z","shell.execute_reply":"2025-01-20T04:28:01.929283Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pykeen.triples.utils import get_entities\nentity_to_id = result.training.entity_to_id\nrelation_to_id = result.training.relation_to_id\n\n\n# Create triples for the test data\ntest_triples = []\nfor index, row in test_df.iterrows():\n    trans_id = f'trans_{index}'\n    test_triples.append((trans_id, 'made_at', row['merchant']))\n    test_triples.append((trans_id, 'belongs_to', row['category']))\n    test_triples.append((trans_id, 'located_in', row['city']))\n    test_triples.append((trans_id, 'in', row['state']))\n    test_triples.append((trans_id, 'in', row['time_of_day_category']))\n    test_triples.append((trans_id, 'job', row['job']))\n    #test_triples.append((trans_id, 'card', row['cc_num']))\ntest_triples = np.array(test_triples)\n\n# Remove malformed entries (if any)\nvalid_test_triples = [triple for triple in test_triples if len(triple) == 3 and all(isinstance(item, str) for item in triple)]\ntest_triples = np.array(valid_test_triples)\n\n\n# Create a TriplesFactory for the test data, handling unknown entities\ntest_tf = TriplesFactory.from_labeled_triples(\n    triples=test_triples,\n    entity_to_id=entity_to_id,  # Use the entity to ID mapping from training\n    relation_to_id=result.training.relation_to_id,  # Use relation mapping from training\n    create_inverse_triples=False, # Assuming you don't need inverse triples\n)\n\n","metadata":{"id":"EfZpyN2dVdH6","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:28:01.930226Z","iopub.status.idle":"2025-01-20T04:28:01.930552Z","shell.execute_reply":"2025-01-20T04:28:01.930428Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pykeen.triples.utils import get_entities\nimport numpy as np\n\n# Access the entity and relation mappings from train_tf\nentity_to_id = train_tf.entity_to_id\nrelation_to_id = train_tf.relation_to_id\n\n# Create triples for the test data\ntest_triples = []\nfor index, row in test_df.iterrows():\n    trans_id = f'trans_{index}'\n    test_triples.append((trans_id, 'made_at', row['merchant']))\n    test_triples.append((trans_id, 'belongs_to', row['category']))\n    test_triples.append((trans_id, 'located_in', row['city']))\n    test_triples.append((trans_id, 'in', row['state']))\n    test_triples.append((trans_id, 'in', row['time_of_day_category']))\n    test_triples.append((trans_id, 'job', row['job']))\n    #test_triples.append((trans_id, 'card', row['cc_num']))\ntest_triples = np.array(test_triples)\n\n# Remove malformed entries (if any)\nvalid_test_triples = [triple for triple in test_triples if len(triple) == 3 and all(isinstance(item, str) for item in triple)]\ntest_triples = np.array(valid_test_triples)\n\n# Create a TriplesFactory for the test data, handling unknown entities\ntest_tf = TriplesFactory.from_labeled_triples(\n    triples=test_triples,\n    entity_to_id=entity_to_id,  # Use the entity to ID mapping from training\n    relation_to_id=relation_to_id,  # Use relation mapping from training\n    create_inverse_triples=False, # Assuming you don't need inverse triples\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:28:01.931933Z","iopub.status.idle":"2025-01-20T04:28:01.932435Z","shell.execute_reply":"2025-01-20T04:28:01.932282Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Get entity embeddings for the test data\n# Get all entity IDs present in the test data\nall_test_entity_ids = set(test_tf.entities_to_ids(get_entities(test_tf.mapped_triples))) # Instead of test_tf.get_entities()\n\n# Filter entity embeddings for entities present in test data\ntest_entity_indices = torch.tensor([test_tf.entity_to_id[entity] for entity in test_tf.entity_to_id if test_tf.entity_to_id[entity] in all_test_entity_ids], device=result.model.device)\ntest_embeddings = result.model.entity_representations[0](indices=test_entity_indices).cpu().detach().numpy()\n\n# Create a DataFrame with embeddings for the test data, using filtered entity IDs\ntest_embedding_df = pd.DataFrame(test_embeddings, index=test_entity_indices.cpu().numpy())\n\n# Merge embeddings with the test DataFrame, using numeric entity IDs\ntest_df['embedding'] = test_df.index.map(lambda x: test_embedding_df.loc[test_tf.entity_to_id.get(f'trans_{x}')].values if test_tf.entity_to_id.get(f'trans_{x}') is not None else None)","metadata":{"id":"IiVu0aagboVa","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:28:01.933085Z","iopub.status.idle":"2025-01-20T04:28:01.933433Z","shell.execute_reply":"2025-01-20T04:28:01.933308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pykeen.triples.utils import get_entities\nimport numpy as np\n\n# Access the entity and relation mappings from train_tf\nentity_to_id = train_tf.entity_to_id\nrelation_to_id = train_tf.relation_to_id\n\n# Create triples for the test data\ntest_triples = []\nfor index, row in test_df.iterrows():\n    trans_id = f'trans_{index}'\n    test_triples.append((trans_id, 'made_at', row['merchant']))\n    test_triples.append((trans_id, 'belongs_to', row['category']))\n    test_triples.append((trans_id, 'located_in', row['city']))\n    test_triples.append((trans_id, 'in', row['state']))\n    test_triples.append((trans_id, 'in', row['time_of_day_category']))\n    test_triples.append((trans_id, 'job', row['job']))\n    #test_triples.append((trans_id, 'card', row['cc_num']))\ntest_triples = np.array(test_triples)\n\n# Remove malformed entries (if any)\nvalid_test_triples = [triple for triple in test_triples if len(triple) == 3 and all(isinstance(item, str) for item in triple)]\ntest_triples = np.array(valid_test_triples)\n\n# Create a TriplesFactory for the test data, handling unknown entities\ntest_tf = TriplesFactory.from_labeled_triples(\n    triples=test_triples,\n    entity_to_id=entity_to_id,  # Use the entity to ID mapping from training\n    relation_to_id=relation_to_id,  # Use relation mapping from training\n    create_inverse_triples=False, # Assuming you don't need inverse triples\n)\n\n# Filter entity embeddings for entities present in test data\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntest_entity_indices = torch.tensor(\n    [test_tf.entity_to_id[entity] for entity in test_tf.entity_to_id if test_tf.entity_to_id[entity] in all_test_entity_ids],\n    device=device\n)\ntest_embeddings = model.entity_representations[0]\n\nprint(test_embeddings)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:28:01.934388Z","iopub.status.idle":"2025-01-20T04:28:01.934860Z","shell.execute_reply":"2025-01-20T04:28:01.934645Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Shape of test_embeddings:\", test_embeddings.shape)\nprint(\"Shape of test_entity_indices:\", test_entity_indices.shape)\n\n# Create a DataFrame with embeddings for the test data, using filtered entity IDs\ntest_embedding_df = pd.DataFrame(test_embeddings, index=test_entity_indices.cpu().numpy())\n\n# Merge embeddings with the test DataFrame, using numeric entity IDs\ntest_df['embedding'] = test_df.index.map(lambda x: test_embedding_df.loc[test_tf.entity_to_id.get(f'trans_{x}')].values if test_tf.entity_to_id.get(f'trans_{x}') is not None else None)\n\nprint(test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:28:01.935763Z","iopub.status.idle":"2025-01-20T04:28:01.936212Z","shell.execute_reply":"2025-01-20T04:28:01.935999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df['gender_encoded'] = le.transform(test_df['gender'])","metadata":{"execution":{"iopub.status.busy":"2025-01-20T04:28:01.937056Z","iopub.status.idle":"2025-01-20T04:28:01.937523Z","shell.execute_reply":"2025-01-20T04:28:01.937329Z"},"trusted":true,"id":"eeDBfzMpRPRN"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.to_csv(\"test_df_new.csv\")","metadata":{"id":"V_wYGQAMWRf_","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:28:01.938226Z","iopub.status.idle":"2025-01-20T04:28:01.938515Z","shell.execute_reply":"2025-01-20T04:28:01.938398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:28:01.939171Z","iopub.status.idle":"2025-01-20T04:28:01.939588Z","shell.execute_reply":"2025-01-20T04:28:01.939398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df_trim=test_df[['amt', 'distance', 'age', 'gender_encoded', 'community', 'time_of_day_category', 'job']]\ntest_df_encoded = pd.get_dummies(test_df_trim, columns=['time_of_day_category', 'job'])\nfor col in features:\n    test_df_encoded[col] = test_df_encoded[col].astype('int')\n\nmissing_cols = set(X_train.columns) - set(test_df_encoded.columns)\nfor col in missing_cols:\n    test_df_encoded[col] = 0\n\ntest_df_encoded = test_df_encoded[X_train.columns]\n\n\nX_test = test_df_encoded\ny_test_pred_proba = model.predict_proba(X_test)[:, 1]\ny_test_pred = (y_test_pred_proba >= best_cutoff).astype(int) # Use the best cutoff from training\n\n# Evaluate the model on the test data\nprint(classification_report(test_df['is_fraud'], y_test_pred)) # Assuming 'is_fraud' column exists in test_df\nprint(confusion_matrix(test_df['is_fraud'], y_test_pred))","metadata":{"execution":{"iopub.status.busy":"2025-01-20T04:28:01.940453Z","iopub.status.idle":"2025-01-20T04:28:01.940892Z","shell.execute_reply":"2025-01-20T04:28:01.940691Z"},"trusted":true,"id":"M8AaDNSjRPRN"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### Use stacking","metadata":{"id":"ca42c0c5"}},{"cell_type":"code","source":"X =  train_df[['amt', 'distance', 'age', 'gender_encoded','community','time_of_day_category','job']]#'predicted_labels'\ny = train_df['is_fraud']\n\n# One-hot encode categorical features\nX_encoded = pd.get_dummies(X, columns=['time_of_day_category', 'job'])\nfeatures = ['gender_encoded', 'community']\nfor col in features:\n    X_encoded[col] = X_encoded[col].astype('int')\n\n# Split the data\nX_train, X_val, y_train, y_val = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:28:01.941562Z","iopub.status.idle":"2025-01-20T04:28:01.941852Z","shell.execute_reply":"2025-01-20T04:28:01.941737Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip install scikit-learn==1.2.2\n#!pip install xgboost==1.7.5\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n\n\nbase_learners = [\n    ('rf',RandomForestClassifier(n_estimators=100)),\n    ('xgb', xgb.XGBClassifier(random_state=42, enable_categorical=True))\n]\n\nmeta_learner = LogisticRegression()\nstacking_model = StackingClassifier(estimators=base_learners, final_estimator=meta_learner)\n\nstacking_model.fit(X_train, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2025-01-20T04:28:01.942453Z","iopub.status.idle":"2025-01-20T04:28:01.942770Z","shell.execute_reply":"2025-01-20T04:28:01.942638Z"},"id":"7a6cb7cd","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#model = xgb.XGBClassifier(early_stopping_rounds=10, eval_metric='auc',enable_categorical=True)\n#model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=True)\n\n# Make predictions on the validation set\ny_val_pred = stacking_model.predict(X_val)\n\nprint(f\"classification report \\n {classification_report(y_val, y_val_pred)}\")\nprint(f\"confusion_matrix \\n {confusion_matrix(y_val, y_val_pred)}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-01-20T04:28:01.943513Z","iopub.status.idle":"2025-01-20T04:28:01.943846Z","shell.execute_reply":"2025-01-20T04:28:01.943713Z"},"id":"97d74d6a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in X_train.columns:\n    if col not in X_test.columns:\n        X_test[col] = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:28:01.944700Z","iopub.status.idle":"2025-01-20T04:28:01.944992Z","shell.execute_reply":"2025-01-20T04:28:01.944873Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test = X_test[X_train.columns]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:28:01.945872Z","iopub.status.idle":"2025-01-20T04:28:01.946220Z","shell.execute_reply":"2025-01-20T04:28:01.946042Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = stacking_model.predict(X_test)\ny_test=test_df['is_fraud']\nprint(f\"classification report \\n {classification_report(y_test, y_pred)}\")\nprint(f\"confusion_matrix \\n {confusion_matrix(y_test, y_pred)}\")","metadata":{"execution":{"iopub.status.busy":"2025-01-20T04:28:01.947518Z","iopub.status.idle":"2025-01-20T04:28:01.947950Z","shell.execute_reply":"2025-01-20T04:28:01.947760Z"},"trusted":true,"id":"A65_i13SRPRQ"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install LIME\n#!pip install lime\n\nimport lime\nimport lime.lime_tabular\nimport numpy as np\n\n# Assuming you have a trained stacking classifier named 'stacking_clf'\n# and your training data is in 'X_train' and 'y_train'\n\n# Create a LIME explainer\nexplainer = lime.lime_tabular.LimeTabularExplainer(\n    training_data=np.array(X_train),\n    feature_names=X_train.columns,\n    class_names=['Not Fraud', 'Fraud'],\n    mode='classification'\n)\n\n# Select an instance to explain\ninstance_idx = 2000\ninstance = X_train.iloc[instance_idx]\n\n# Generate explanation\nexp = explainer.explain_instance(\n    data_row=instance,\n    predict_fn=stacking_model.predict_proba\n)\n\n# Display explanation\nexp.show_in_notebook(show_table=True, show_all=False)","metadata":{"id":"DMmsRB4wRPRR","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:28:01.948830Z","iopub.status.idle":"2025-01-20T04:28:01.949293Z","shell.execute_reply":"2025-01-20T04:28:01.949071Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"With upsampling of minority class","metadata":{}},{"cell_type":"code","source":"\ndf_majority = train_df[train_df.is_fraud==0]\ndf_minority = train_df[train_df.is_fraud==1]\n\n# Upsample minority class\nfrom sklearn.utils import resample\ndf_minority_upsampled = resample(df_minority,\n                                 replace=True,     # sample with replacement\n                                 n_samples=int(0.05*len(train_df)),    # to match majority class\n                                 random_state=123) # reproducible results\n\n# Combine majority class with upsampled minority class\ntrain_df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n\n# Display new class counts\ntrain_df_upsampled.is_fraud.value_counts()\n\n","metadata":{"execution":{"iopub.status.busy":"2025-01-20T04:28:01.950155Z","iopub.status.idle":"2025-01-20T04:28:01.950586Z","shell.execute_reply":"2025-01-20T04:28:01.950400Z"},"id":"7504f99c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nle = LabelEncoder()\ntrain_df_upsampled['gender_encoded'] = le.fit_transform(train_df_upsampled['gender'])\n\nX =  train_df_upsampled[['amt', 'distance', 'age', 'gender_encoded','community','time_of_day_category','job']]\ny = train_df_upsampled['is_fraud']\nX_encoded = pd.get_dummies(X, columns=['time_of_day_category', 'job'])\nX_train, X_val, y_train, y_val = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n\n# Initialize and train the XGBoost model\nmodel = xgb.XGBClassifier(random_state=42, enable_categorical=True)\nmodel.fit(X_train, y_train)\ny_val_pred = model.predict(X_val)\n\nprint(f\"classification report \\n {classification_report(y_val, y_val_pred)}\")\nprint(f\"confusion_matrix \\n {confusion_matrix(y_val, y_val_pred)}\")\n\nxgb.plot_importance(model)\nplt.title('Feature Importance')\nplt.show()\n\ny_val_pred_proba = model.predict_proba(X_val)[:, 1]\ncutoffs = np.linspace(0, 1, 101)\nprecision = []\nrecall = []\nf1 = []\n\nfor cutoff in cutoffs:\n    y_val_pred = (y_val_pred_proba >= cutoff).astype(int)\n    precision.append(precision_score(y_val, y_val_pred))\n    recall.append(recall_score(y_val, y_val_pred))\n    f1.append(f1_score(y_val, y_val_pred))\n\nplt.figure(figsize=(10, 6))\nplt.plot(cutoffs, precision, label='Precision')\nplt.plot(cutoffs, recall, label='Recall')\nplt.plot(cutoffs, f1, label='F1 Score')\nplt.xlabel('Cutoff Probability')\nplt.ylabel('Score')\nplt.title('Precision, Recall, and F1 Score vs. Cutoff Probability')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nbest_cutoff = cutoffs[np.argmax(f1)]\nprint(f\"Best cutoff for F1 Score: {best_cutoff}\")\n\ny_val_pred = (y_val_pred_proba >= best_cutoff).astype(int)\nprint(classification_report(y_val,y_val_pred))\n\nxgb.plot_importance(model)\nplt.title('Feature Importance')\nplt.show()\n\nprint(\"Testing\\n\")\ntest_df_trim=test_df[['amt', 'distance', 'age', 'gender_encoded', 'community', 'time_of_day_category', 'job']]\ntest_df_encoded = pd.get_dummies(test_df_trim, columns=['time_of_day_category', 'job'])\n\nmissing_cols = set(X_train.columns) - set(test_df_encoded.columns)\nfor col in missing_cols:\n    test_df_encoded[col] = 0\n\ntest_df_encoded = test_df_encoded[X_train.columns]\n\n\nX_test = test_df_encoded\ny_test_pred_proba = model.predict_proba(X_test)[:, 1]\ny_test_pred = (y_test_pred_proba >= best_cutoff).astype(int) # Use the best cutoff from training\n\n# Evaluate the model on the test data\nprint(classification_report(test_df['is_fraud'], y_test_pred)) # Assuming 'is_fraud' column exists in test_df\nprint(confusion_matrix(test_df['is_fraud'], y_test_pred))","metadata":{"execution":{"iopub.status.busy":"2025-01-20T04:28:01.951464Z","iopub.status.idle":"2025-01-20T04:28:01.951893Z","shell.execute_reply":"2025-01-20T04:28:01.951706Z"},"id":"hPOnZGYY1h0i","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom pykeen.models import TransE\nfrom pykeen.triples import TriplesFactory\n\n# Initialize the model with the same parameters used during training\nmodel = TransE(\n    triples_factory=train_tf,  # Add the triples_factory argument\n    embedding_dim=50,  # Ensure this matches the embedding dimension used during training\n)\n\n\n# Load the saved state dictionary\nmodel.load_state_dict(torch.load('transe_model.pt'))\n\n# Move the model to GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\n# Example: Get embeddings for a specific entity\nentity_id = train_tf.entity_to_id['trans_0']  # Replace 'trans_0' with your entity ID\nentity_embedding = model.entity_representations[0]\n\nprint(entity_embedding)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:28:01.953438Z","iopub.status.idle":"2025-01-20T04:28:01.953961Z","shell.execute_reply":"2025-01-20T04:28:01.953752Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"entity_to_id = model.training.entity_to_id\n\n# Create triples for the test data\ntest_triples = []\nfor index, row in test_df.iterrows():\n    trans_id = f'trans_{index}'\n    test_triples.append((trans_id, 'made_at', row['merchant']))\n    test_triples.append((trans_id, 'belongs_to', row['category']))\n    test_triples.append((trans_id, 'located_in', row['city']))\n    test_triples.append((trans_id, 'in', row['state']))\n    test_triples.append((trans_id, 'in', row['time_of_day_category']))\n    test_triples.append((trans_id, 'job', row['job']))\n    #test_triples.append((trans_id, 'card', row['cc_num']))\ntest_triples = np.array(test_triples)\n\n# Remove malformed entries (if any)\nvalid_test_triples = [triple for triple in test_triples if len(triple) == 3 and all(isinstance(item, str) for item in triple)]\ntest_triples = np.array(valid_test_triples)\n\n# Create a TriplesFactory for the test data, handling unknown entities\ntest_tf = TriplesFactory.from_labeled_triples(\n    triples=test_triples,\n    entity_to_id=entity_to_id,  # Use the entity to ID mapping from training\n    relation_to_id=result.training.relation_to_id,  # Use relation mapping from training\n    create_inverse_triples=False,  # Assuming you don't need inverse triples\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T04:28:01.954852Z","iopub.status.idle":"2025-01-20T04:28:01.955209Z","shell.execute_reply":"2025-01-20T04:28:01.955063Z"}},"outputs":[],"execution_count":null}]}